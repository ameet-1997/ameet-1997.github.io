---
layout: page
permalink: /research/
title: Research
description: I'm interested in multiple research areas and my papers can be broadly categorized as follows.
nav: true
nav_order: 2
---

#### Efficient NLP
With large language models becoming larger than ever, I believe efficiency of the architectures should be a constant question.
I am interested in building parameter-efficient and fast throughput architectures [<a href="https://arxiv.org/pdf/2211.16634">[1]</a>,<a href="https://arxiv.org/pdf/2302.12441.pdf">[2]</a>]
In an earlier work, we showed that even existing architectures like the Transformer can be made to converge significantly faster during pre-training by guiding their attention patterns <a href="https://arxiv.org/pdf/2010.02399.pdf">[3]</a>.

#### Reinforcement learning
List papers here.