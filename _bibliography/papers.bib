@inproceedings{reddy2019figurenet,
  abbr={QA},
  abstract={Deep Learning has managed to push boundaries in a wide variety of tasks. One area of interest is to tackle problems in reasoning and understanding, with an aim to emulate human intelligence. In this work, we describe a deep learning model that addresses the reasoning task of question-answering on categorical plots. We introduce a novel architecture FigureNet, that learns to identify various plot elements, quantify the represented values and determine a relative ordering of these statistical values. We test our model on the FigureQA dataset which provides images and accompanying questions for scientific plots like bar graphs and pie charts, augmented with rich annotations. Our approach outperforms the state-of-the-art Relation Networks baseline by approximately 7% on this dataset, with a training time that is over an order of magnitude lesser.},
  bibtex_show={true},
  preview={Figurenet.png},
  pdf={https://arxiv.org/pdf/1806.04655.pdf},
  title={Figurenet: A deep learning model for question-answering on scientific plots},
  author={Reddy, Revanth and Ramesh, Rahul and Deshpande, Ameet and Khapra, Mitesh M},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

@inproceedings{deshpande2020guiding,
  abbr={Analysis},
  abstract={In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.},
  bibtex_show={true},
  preview={Guiding.png},
  pdf={https://arxiv.org/pdf/2010.02399.pdf},
  title={Guiding Attention for Self-Supervised Learning with Transformers},
  author={Deshpande, Ameet and Narasimhan, Karthik},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4676--4686},
  year={2020}
}

@inproceedings{deshpande2022bert,
  abbr={Analysis,Multilingual},
  abstract={While recent work on multilingual language models has demonstrated their capacity for cross-lingual zero-shot transfer on downstream tasks, there is a lack of consensus in the community as to what shared properties between languages enable such transfer. Analyses involving pairs of natural languages are often inconclusive and contradictory since languages simultaneously differ in many linguistic aspects. In this paper, we perform a large-scale empirical study to isolate the effects of various linguistic properties by measuring zero-shot transfer between four diverse natural languages and their counterparts constructed by modifying aspects such as the script, word order, and syntax. Among other things, our experiments show that the absence of sub-word overlap significantly affects zero-shot transfer when languages differ in their word order, and there is a strong correlation between transfer performance and word embedding alignment between languages (e.g., R=0.94 on the task of NLI). Our results call for focus in multilingual models on explicitly improving word embedding alignment between languages rather than relying on its implicit emergence.},
  bibtex_show={true},
  preview={Multilingual_1.png},
  pdf={https://arxiv.org/pdf/2110.14782.pdf},
  title={When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer},
  author={Deshpande, Ameet and Talukdar, Partha and Narasimhan, Karthik},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3610--3623},
  year={2022}
}

@article{hanjie2022semantic,
  abbr={Zero-shot,NL-supervision},
  abstract={In this paper, we propose Semantic Supervision (SemSup) - a unified paradigm for training classifiers that generalize over output spaces. In contrast to standard classification, which treats classes as discrete symbols, SemSup represents them as dense vector features obtained from descriptions of classes (e.g., "The cat is a small carnivorous mammal"). This allows the output space to be unbounded (in the space of descriptions) and enables models to generalize both over unseen inputs and unseen outputs (e.g. "The aardvark is a nocturnal burrowing mammal with long ears"). Specifically, SemSup enables four types of generalization, to -- (1) unseen class descriptions, (2) unseen classes, (3) unseen super-classes, and (4) unseen tasks. Through experiments on four classification datasets across two variants (multi-class and multi-label), two input modalities (text and images), and two output description modalities (text and JSON), we show that our SemSup models significantly outperform standard supervised models and existing models that leverage word embeddings over class names. For instance, our model outperforms baselines by 40% and 15% precision points on unseen descriptions and classes, respectively, on a news categorization dataset (RCV1). SemSup can serve as a pathway for scaling neural models to large unbounded output spaces and enabling better generalization and model reuse for unseen tasks and domains.},
  bibtex_show={true},
  preview={SemSup.png},
  pdf={https://arxiv.org/pdf/2202.13100},
  title={Semantic supervision: Enabling generalization over output spaces},
  author={Hanjie, Austin W and Deshpande, Ameet and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2202.13100},
  year={2022}
}

@article{deshpande2020evaluating,
  abbr={GANs,Retrieval},
  abstract={Recent advances in Generative Adversarial Networks (GANs) have resulted in its widespread applications to multiple domains. A recent model, IRGAN, applies this framework to Information Retrieval (IR) and has gained significant attention over the last few years. In this focused work, we critically analyze multiple components of IRGAN, while providing experimental and theoretical evidence of some of its shortcomings. Specifically, we identify issues with the constant baseline term in the policy gradients optimization and show that the generator harms IRGAN's performance. Motivated by our findings, we propose two models influenced by self-contrastive estimation and co-training which outperform IRGAN on two out of the three tasks considered.},
  bibtex_show={true},
  preview={IRGAN.png},
  pdf={https://arxiv.org/pdf/2010.00722},
  title={Evaluating a generative adversarial framework for information retrieval},
  author={Deshpande, Ameet and Khapra, Mitesh M},
  journal={arXiv preprint arXiv:2010.00722},
  year={2020}
}

@article{deshpande2020sentiment,
  abbr={RL},
  abstract={While reinforcement learning (RL) has been successful in natural language processing (NLP) domains such as dialogue generation and text-based games, it typically faces the problem of sparse rewards that leads to slow or no convergence. Traditional methods that use text descriptions to extract only a state representation ignore the feedback inherently present in them. In text-based games, for example, descriptions like "Good Job! You ate the food" indicate progress, and descriptions like "You entered a new room" indicate exploration. Positive and negative cues like these can be converted to rewards through sentiment analysis. This technique converts the sparse reward problem into a dense one, which is easier to solve. Furthermore, this can enable reinforcement learning without rewards, in which the agent learns entirely from these intrinsic sentiment rewards. This framework is similar to intrinsic motivation, where the environment does not necessarily provide the rewards, but the agent analyzes and realizes them by itself. We find that providing dense rewards in text-based games using sentiment analysis improves performance under some conditions.},
  bibtex_show={true},
  preview={SARL.png},
  pdf={https://arxiv.org/pdf/2010.02316},
  title={Sentiment Analysis for Reinforcement Learning},
  author={Deshpande, Ameet and Fleisig, Eve},
  journal={arXiv preprint arXiv:2010.02316},
  year={2020}
}

@article{deshpande2018improvements,
  abbr={RL},
  abstract={Sparse reward problems are one of the biggest challenges in Reinforcement Learning. Goal-directed tasks are one such sparse reward problems where a reward signal is received only when the goal is reached. One promising way to train an agent to perform goal-directed tasks is to use Hindsight Learning approaches. In these approaches, even when an agent fails to reach the desired goal, the agent learns to reach the goal it achieved instead. Doing this over multiple trajectories while generalizing the policy learned from the achieved goals, the agent learns a goal conditioned policy to reach any goal. One such approach is Hindsight Experience replay which uses an off-policy Reinforcement Learning algorithm to learn a goal conditioned policy. In this approach, a replay of the past transitions happens in a uniformly random fashion. Another approach is to use a Hindsight version of the policy gradients to directly learn a policy. In this work, we discuss different ways to replay past transitions to improve learning in hindsight experience replay focusing on prioritized variants in particular. Also, we implement the Hindsight Policy gradient methods to robotic tasks.},
  bibtex_show={true},
  preview={Hindsight.png},
  pdf={https://arxiv.org/pdf/1809.06719},
  title={Improvements on hindsight learning},
  author={Deshpande, Ameet and Sarma, Srikanth and Jha, Ashutosh and Ravindran, Balaraman},
  journal={arXiv preprint arXiv:1809.06719},
  year={2018}
}