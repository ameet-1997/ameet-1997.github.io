@inproceedings{reddy2019figurenet,
  abbr={QA},
  abstract={Deep Learning has managed to push boundaries in a wide variety of tasks. One area of interest is to tackle problems in reasoning and understanding, with an aim to emulate human intelligence. In this work, we describe a deep learning model that addresses the reasoning task of question-answering on categorical plots. We introduce a novel architecture FigureNet, that learns to identify various plot elements, quantify the represented values and determine a relative ordering of these statistical values. We test our model on the FigureQA dataset which provides images and accompanying questions for scientific plots like bar graphs and pie charts, augmented with rich annotations. Our approach outperforms the state-of-the-art Relation Networks baseline by approximately 7% on this dataset, with a training time that is over an order of magnitude lesser.},
  bibtex_show={true},
  preview={Figurenet.png},
  pdf={https://arxiv.org/pdf/1806.04655.pdf},
  title={Figurenet: A deep learning model for question-answering on scientific plots},
  author={Reddy, Revanth and Ramesh, Rahul and Deshpande, Ameet and Khapra, Mitesh M},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

@inproceedings{deshpande2020guiding,
  abbr={Analysis},
  abstract={In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.},
  bibtex_show={true},
  author+an={1=highlight},
  title={Guiding Attention for Self-Supervised Learning with Transformers},
  author={Deshpande, Ameet and Narasimhan, Karthik},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4676--4686},
  year={2020}
}